<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Daniel's Blog</title>
    <description>Daniel's Blogs</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 26 Feb 2019 12:23:18 +0800</pubDate>
    <lastBuildDate>Tue, 26 Feb 2019 12:23:18 +0800</lastBuildDate>
    <generator>Jekyll v3.6.2</generator>
    
      <item>
        <title>Spark Kmeans</title>
        <description>&lt;h3 id=&quot;spark-k-means&quot;&gt;spark k-means&lt;/h3&gt;

&lt;p&gt;Seed 代表一開始初始化 random 的點．K 表示 cluster 的數量．setInitMode 目前支持 random 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;k-means||&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;k-means||&lt;/code&gt; 是 k-means++．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val spark = SparkSession.builder().master(&quot;local[*]&quot;).appName(&quot;testtest&quot;).config(&quot;spark.cassandra.connection.host&quot;, &quot;192.168.6.31&quot;).getOrCreate()
val dataset = spark.createDataFrame(Seq(
  (0, Vectors.sparse(6, Seq((0, 1.0), (1, 1.0), (2, 1.0)))), // [1.0 , 1.0 , 1.0 , 0   , 0   , 0  ]
  (1, Vectors.sparse(6, Seq((2, 1.0), (3, 1.0), (4, 1.0)))), // [0   , 0   , 1.0 , 1.0 , 1.0 , 0  ]
  (2, Vectors.sparse(6, Seq((0, 1.0), (2, 1.0), (4, 1.0)))), // [1.0 , 0   , 1.0 , 0   , 1.0 , 0  ]
  (3, Vectors.sparse(6, Seq((1, 1.0), (3, 1.0), (4, 1.0)))), // [0   , 1.0 , 0   , 1.0 , 1.0 , 0  ]
  (4, Vectors.sparse(6, Seq((1, 1.0), (3, 1.0), (5, 1.0)))), // [0   , 1.0 , 0   , 1.0 , 0   , 1.0]
  (5, Vectors.sparse(6, Seq((2, 1.0), (3, 1.0), (5, 1.0)))), // [0   , 0   , 1.0 , 1.0 , 0   , 1.0]
  (6, Vectors.sparse(6, Seq((1, 1.0), (2, 1.0), (4, 1.0)))), // [0   , 1.0 , 1.0 , 0   , 1.0 , 0  ]
  (7, Vectors.sparse(6, Seq((0, 1.0), (1, 1.0), (3, 1.0))))  // [1.0 , 1.0 , 0   , 1.0 , 0   , 0  ]
)).toDF(&quot;id&quot;, &quot;features&quot;)

// Trains a k-means model.
val kmeans = new KMeans().setK(3).setSeed(1L)

val model = kmeans.fit(dataset)

// Make predictions
val predictions = model.transform(dataset)

// Evaluate clustering by computing Silhouette score
val evaluator = new ClusteringEvaluator()

val silhouette = evaluator.evaluate(predictions)
println(s&quot;Silhouette with squared euclidean distance = $silhouette&quot;)

// Shows the result.
println(&quot;Cluster Centers: &quot;)
model.clusterCenters.foreach(println)

predictions.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;印出結果 :&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Silhouette with squared euclidean distance = 0.11111111111111134
Cluster Centers: 
[0.3333333333333333,0.6666666666666666,0.3333333333333333,1.0,0.0,0.6666666666666666]
[0.3333333333333333,0.3333333333333333,0.6666666666666666,0.6666666666666666,1.0,0.0]
[0.5,1.0,1.0,0.0,0.5,0.0]
+---+--------------------+----------+
| id|            features|prediction|
+---+--------------------+----------+
|  0|(6,[0,1,2],[1.0,1...|         2|
|  1|(6,[2,3,4],[1.0,1...|         1|
|  2|(6,[0,2,4],[1.0,1...|         1|
|  3|(6,[1,3,4],[1.0,1...|         1|
|  4|(6,[1,3,5],[1.0,1...|         0|
|  5|(6,[2,3,5],[1.0,1...|         0|
|  6|(6,[1,2,4],[1.0,1...|         2|
|  7|(6,[0,1,3],[1.0,1...|         0|
+---+--------------------+----------+

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;實際在餵給 KMeans 的資料集時，會需要一個 features 的欄位，可透過 Vectors.dense 來將 Array[Double] 轉成 Vector．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;seedPersons.map(r =&amp;gt; ( r._2._1 ,Vectors.dense(r._2._2.toArray))).toDF(&quot;id&quot;, &quot;features&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val outputPersons = 100
//10 count
val testpersonSeed = &quot;hash:9ef58b79-8aab-4e5b-bcf5-b8974991e599,hash:cd5d9c99-393b-4814-8f62-9aebcc31bd21,hash:a0c95fa1-fc44-44ff-b4ae-c407cd53f292,hash:a2ada011-53a1-4d27-9885-dbfd2b0cb969,hash:ce1283e2-a72a-4377-86f5-c03774f00641,hash:4241c150-cde7-4cb7-9306-10946cdbb639,hash:be9fcd09-a1f5-467a-9065-82a89c86af83,hash:48f3d940-f0bd-4300-9f8a-33c29c56ace6,hash:35dc9c3d-f04d-4b27-a5ce-8180ea9705ec,hash:57258600-55c5-4d94-8d15-8f754d8d14bc&quot;

val inputIdList = testpersonSeed
val spark = SparkSession.builder()
      .appName(&quot;LookLikePersonLSH&quot;)
      .master(&quot;local[*]&quot;)
      .config(&quot;spark.cassandra.connection.host&quot;, &quot;192.168.6.31,192.168.6.32,192.168.6.33&quot;)
      .config(&quot;spark.cassandra.auth.username&quot;, &quot;miuser&quot;)
      .config(&quot;spark.cassandra.auth.password&quot;, &quot;mimimi123&quot;)
      .getOrCreate()

import spark.implicits._

val allPersons = spark.sparkContext.cassandraTable[(String,Seq[Double])](&quot;miks2&quot;,&quot;testpersonlabelvector&quot;)
  .select(&quot;id&quot;,&quot;labelvector&quot;).keyBy[Tuple1[String]](&quot;id&quot;)

val seedPersons = spark.sparkContext.cassandraTable[(String,Seq[Double])](&quot;miks2&quot;,&quot;testpersonlabelvector&quot;)
  .select(&quot;id&quot;,&quot;labelvector&quot;).where(&quot;id in ? &quot; , testpersonSeed.split(&quot;,&quot;).toSeq).keyBy[Tuple1[String]](&quot;id&quot;)

val tranDataSet = seedPersons.map(r =&amp;gt; ( r._2._1 ,Vectors.dense(r._2._2.toArray))).toDF(&quot;id&quot;, &quot;features&quot;)

val kmeans = new KMeans().setK(5).setSeed(5L)
val model = kmeans.fit(tranDataSet)

val seedKmodel = spark.sparkContext.parallelize(model.clusterCenters)
val resultIdList = seedKmodel.cartesian(allPersons)
  .map(ds =&amp;gt; (ds._2._1._1 , LabelVectorUtil().cosineSimilarity(ds._1.toArray , ds._2._2._2)))
  .sortBy(_._2 , false).take(outputPersons)

spark.sparkContext.parallelize(resultIdList).saveAsTextFile(&quot;/enrich/tempResult/KmeansLookLikePerson&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Mon, 25 Feb 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/spark/2019/02/25/spark-kmeans.html</link>
        <guid isPermaLink="true">http://localhost:4000/spark/2019/02/25/spark-kmeans.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>Spark Querycassandra</title>
        <description>&lt;h3 id=&quot;spark-query-cassandra&quot;&gt;spark query Cassandra&lt;/h3&gt;

&lt;p&gt;建立 Cassandra KEYSPACE 以及 TABLE&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE KEYSPACE miks2 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1' }  AND durable_writes = true;

CREATE TABLE miks2.testpersonlabelvector(
id text,
labelvector list&amp;lt;double&amp;gt;,
PRIMARY KEY(id)
) ;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;看 KEYSPACE 狀態&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nodetool status miks2 ;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;使用 spark Cassandra connector 對 cassandra 做查詢，詳細資料可以參考&lt;a href=&quot;https://github.com/datastax/spark-cassandra-connector&quot;&gt;spark-cassandra-connector&lt;/a&gt;．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import com.datastax.spark.connector._

val spark = SparkSession.builder()
  .appName(&quot;LookLikePersonLSH&quot;)
  .master(&quot;local[*]&quot;)
  .config(&quot;spark.cassandra.connection.host&quot;, &quot;192.168.6.31,192.168.6.32,192.168.6.33&quot;)
  .config(&quot;spark.cassandra.auth.username&quot;, &quot;miuser&quot;)
  .config(&quot;spark.cassandra.auth.password&quot;, &quot;mimimi123&quot;)
  .getOrCreate()

val allPersons = spark.sparkContext.cassandraTable[(String,Seq[Double])](&quot;miks2&quot;,&quot;testpersonlabelvector&quot;)
      .select(&quot;id&quot;,&quot;labelvector&quot;).keyBy[Tuple1[String]](&quot;id&quot;)

val testpersonSeed = &quot;hash:52f479d0-126c-4ccd-bf3b-3d99a6d3fec0,hash:79bdf00a-b739-42a5-bb3c-c55315011b52,hash:08565f43-5e61-4d4f-ac5c-d2fa2416e8f0&quot;
val seedPersons = spark.sparkContext.cassandraTable[(String,Seq[Double])](&quot;miks2&quot;,&quot;testpersonlabelvector&quot;)
      .select(&quot;id&quot;,&quot;labelvector&quot;).where(&quot;id in ? &quot; , testpersonSeed.split(&quot;,&quot;).toSeq).keyBy[Tuple1[String]](&quot;id&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;計算結果並存到 HDFS&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val tempList = seedPersons.cartesian(allPersons).map(ds =&amp;gt; (ds._1._1._1 , (ds._2._1._1 , LabelVectorUtil().cosineSimilarity(ds._1._2._2 , ds._2._2._2))))
val takeCount = (outputPersons / allPersons.getNumPartitions)
val resultIdList = tempList.mapPartitions(rowit =&amp;gt; {
    val tempDatas = rowit.toSeq.groupBy(_._1)
    val takeCnt = (takeCount / tempDatas.size) + 1
    tempDatas.map(personInfos =&amp;gt; {
      personInfos._2.sortWith(_._2._2 &amp;gt; _._2._2).take(takeCnt)
    }).flatten.toIterator
  }).cache()
resultIdList.saveAsTextFile(&quot;/enrich/tempResult/LookLikePersons&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Sat, 23 Feb 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/spark/2019/02/23/spark-queryCassandra.html</link>
        <guid isPermaLink="true">http://localhost:4000/spark/2019/02/23/spark-queryCassandra.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>Spark Repartitionandsortwithinpartitions</title>
        <description>&lt;h3 id=&quot;spark-rdd-repartitionandsortwithinpartitions-測試&quot;&gt;spark RDD repartitionAndSortWithinPartitions 測試&lt;/h3&gt;

&lt;p&gt;透過 RDD 的 repartitionAndSortWithinPartitions 可以把 RDD 重新 repartition 並在新的 partition 進行排序．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val spark = SparkSession.builder().master(&quot;local[*]&quot;).appName(&quot;testtest&quot;).getOrCreate()
val data = spark.sparkContext.parallelize(Seq(5,2,1,66,3,21,52,35,10,88,7,28))
println(&quot;bdfore : &quot; + data.getNumPartitions)
val partitionData = data.zipWithIndex.repartitionAndSortWithinPartitions(new HashPartitioner(3))
println(&quot;after : &quot; + partitionData.getNumPartitions)
val accum = spark.sparkContext.longAccumulator(&quot;My Accumulator&quot;)
partitionData.mapPartitions(it =&amp;gt; {
  val topData = it.toSeq.iterator
  val dataStr = topData.mkString(&quot;;&quot;)
  dataStr.split(&quot;;&quot;).foreach(d =&amp;gt; accum.add(d.split(&quot;,&quot;)(0).replace(&quot;(&quot;,&quot;&quot;).toInt))
  println(accum.value + &quot; # &quot; + dataStr)
  topData
}).collect()
println(&quot;accum value is &quot; + accum.value)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;根據下列的結果可以看出，原來的 partition 是 8，repartition 後就變成給定的 3，partition 內容 :&lt;br /&gt;
(2,1);(5,0);(35,7) 在同一個 partition&lt;br /&gt;
(3,4);(21,5);(66,3) 在同一個 partition&lt;br /&gt;
(1,2);(7,10);(10,8);(28,11);(52,6);(88,9) 在同一個 partition&lt;br /&gt;
透過 spaark 的 Accumulator 可以看出每個 partition 的狀況，以及最後算出來的結果．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bdfore : 8
after : 3
42 # (2,1);(5,0);(35,7)
90 # (3,4);(21,5);(66,3)
186 # (1,2);(7,10);(10,8);(28,11);(52,6);(88,9)
accum value is 318
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;接著只要修改這一行，就可以達到 top N 的 效果．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val topData = it.toSeq.take(2).iterator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;就會取各自 partition 的前 2 個 element．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;7 # (2,1);(5,0)
24 # (3,4);(21,5)
8 # (1,2);(7,10)
accum value is 39
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;如果是要針對所有的元素排序可以把 HashPartitioner 的值設 1，表示所有的元素都會在同一個 partition 裡．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val partitionData = data.zipWithIndex.repartitionAndSortWithinPartitions(new HashPartitioner(1))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;結果會是&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bdfore : 8
after : 1
3 # (1,2);(2,1)
accum value is 3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;根據-rdd-的-key-做-partition&quot;&gt;根據 RDD 的 Key 做 partition&lt;/h3&gt;

&lt;p&gt;假設有下列資料，希望根據資料的 key 來分成各自的 spark partition．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val datas = Seq(
  (1,Seq((1,2),(1,1))) ,
  (2,Seq((2,3),(2,2))) ,
  (3,Seq((4,5),(3,3))) ,
  (1,Seq((6,7),(4,4)))
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;先來看透過 spark parallelize 會怎麼分 partition．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val drdd = spark.sparkContext.parallelize(datas)
println(&quot;a-&amp;gt; &quot; + drdd.getNumPartitions)
val taccum = spark.sparkContext.longAccumulator(&quot;My Accumulator&quot;)
drdd.foreachPartition(it =&amp;gt; {
  it.foreach(t =&amp;gt; {
    taccum.add(t._1)
    println(taccum.value + &quot; ; &quot; + t._2.mkString(&quot;,&quot;))
  })
})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;總共會分成 8 個 partition，但並不是我們要的．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;a-&amp;gt; 8
1 ; (6,7),(4,4)
1 ; (1,2),(1,1)
2 ; (2,3),(2,2)
3 ; (4,5),(3,3)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;所以透過 groupByKey 再 partitionBy 根據 key 的人數重新 partition．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val keycount = drdd.groupByKey.count().toInt
val repartitionRdd = drdd.groupByKey.partitionBy(new HashPartitioner(keycount))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;println(&quot;keycount : &quot; + keycount)
println(&quot;b-&amp;gt; &quot; + repartitionRdd.getNumPartitions)
val accum = spark.sparkContext.longAccumulator(&quot;My Accumulator&quot;)
repartitionRdd.foreachPartition(it =&amp;gt; {
  it.foreach(t =&amp;gt; {
    accum.add(t._1)
    println(accum.value + &quot; ; &quot; + t._2.mkString(&quot;,&quot;))
  })
})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;結果會是&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;keycount : 3
b-&amp;gt; 3
1 ; List((1,2), (1,1)),List((6,7), (4,4))
2 ; List((2,3), (2,2))
3 ; List((4,5), (3,3))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Fri, 22 Feb 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/spark/2019/02/22/spark-repartitionAndSortWithinPartitions.html</link>
        <guid isPermaLink="true">http://localhost:4000/spark/2019/02/22/spark-repartitionAndSortWithinPartitions.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>Spark Lsh</title>
        <description>&lt;h3 id=&quot;spark-lsh-實作&quot;&gt;spark LSH 實作&lt;/h3&gt;

&lt;p&gt;建立一組 index 並 shuffle．透過 Stream.range 建立一組 1 到 10 的數字，並透過 random swap 的方式打亂．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val suffleIndex = shuffleIndex(Stream.range(1,10).toArray)

def shuffleIndex(arr: Array[Int]): Array[Int] = {
  val rand = new Random()
  for(i &amp;lt;- arr.size to 1 by -1) {
    swap(arr , (i-1) , rand.nextInt(i))
  }
  arr
}

def swap(arr: Array[Int], i:Int ,j:Int): Array[Int] = {
  val tmp = arr(i)
  arr(i) = arr(j)
  arr(j) = tmp
  arr
} 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;輸出結果會是&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;println(suffleIndex.mkString(&quot;,&quot;)) // 6,2,1,7,5,8,3,9,4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;建立 label 的查找順序查看該 label 是否有值，第 1 個找的是 label 8 的值，第 2 個找的是 label 2 的值，以此類推…&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val findLabelSequence = shuffleIndex(Stream.range(1,10).toArray).zipWithIndex.sortWith(_._1 &amp;lt; _._1).map(i =&amp;gt; (i._1 , (i._2 + 1)))
println(findLabelSequence.mkString(&quot;,&quot;)) // (1,8),(2,2),(3,4),(4,9),(5,3),(6,1),(7,5),(8,6),(9,7)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;假設有這些 label 及分數，透過 map 的方式將 Seq[string] 轉成 Map[Int,Double]&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val labels = Seq(
  &quot;1:0.5,4:0.3,8:0.4&quot; ,
  &quot;2:0.7,7:0.9&quot;
)
val personMaps = labels.map(_.split(&quot;,&quot;).map(
  linfo =&amp;gt; {
    (linfo.split(&quot;:&quot;)(0).toInt , linfo.split(&quot;:&quot;)(1).toDouble)
  }
).toMap)
personMaps.foreach(println(_))

// person1 : Map(1 -&amp;gt; 0.5, 4 -&amp;gt; 0.3, 8 -&amp;gt; 0.4)
// person2 : Map(2 -&amp;gt; 0.7, 7 -&amp;gt; 0.9)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;接著只要照剛剛的 findLabelSequence 順序找看看該 Key 是否存在，第一個找到的話就變成該 label 的一個 signatureIndex．&lt;br /&gt;
person1 根據 findLabelSequence 算的 signatureIndex 會是 8．&lt;br /&gt;
person2 根據 findLabelSequence 算的 signatureIndex 會是 2．&lt;br /&gt;
接著可以建立 signature matrix [8] 和 [2]，接著只要有新的 person(person3) 進來，如果透過 findLabelSequence 算出來的值是 8，就可以把該 person 放到 [8] 這著 bucket 裡．&lt;br /&gt;
代表 person1 和 person3 是屬於同一類型的人．&lt;br /&gt;
接著繼續實作…&lt;br /&gt;
給一組 Map (Map(1 -&amp;gt; 0.5, 4 -&amp;gt; 0.3, 8 -&amp;gt; 0.4)) 和 findLabelSequence (Seq((1,8),(2,2),(3,4),(4,9),(5,3),(6,1),(7,5),(8,6),(9,7))) 來找到 signatureIndex&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def findSignatureIndex(labels:Map[Int,Double],findLabelSequence:Seq[Tuple2[Int,Int]]):Int = {
    println(&quot;labels -&amp;gt; &quot; + labels)
    println(&quot;findLabelSequence -&amp;gt; &quot; + findLabelSequence)
	val resultList = findLabelSequence.filter(s =&amp;gt; labels.contains(s._2)).take(1)
	if(resultList.isEmpty) {
	  0
	} else {
	  resultList(0)._2
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;找出每個人的 signatureIndex&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val labels = Seq(
  &quot;1:0.5,4:0.3,8:0.4&quot; ,
  &quot;2:0.7,7:0.9&quot;
)

val personMaps = labels.map(_.split(&quot;,&quot;).map(
  linfo =&amp;gt; {
    (linfo.split(&quot;:&quot;)(0).toInt , linfo.split(&quot;:&quot;)(1).toDouble)
  }
).toMap)
val findLabelSequence = shuffleIndex(Stream.range(1,10).toArray).zipWithIndex.sortWith(_._1 &amp;lt; _._1).map(i =&amp;gt; (i._1 , (i._2 + 1)))
val signatureIndex = personMaps.map(m =&amp;gt; findSignatureIndex(m , findLabelSequence))
signatureIndex.foreach(println(_))

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;執行結果&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;labels -&amp;gt; Map(1 -&amp;gt; 0.5, 4 -&amp;gt; 0.3, 8 -&amp;gt; 0.4)
findLabelSequence -&amp;gt; WrappedArray((1,2), (2,9), (3,8), (4,1), (5,5), (6,6), (7,3), (8,4), (9,7))
labels -&amp;gt; Map(2 -&amp;gt; 0.7, 7 -&amp;gt; 0.9)
findLabelSequence -&amp;gt; WrappedArray((1,2), (2,9), (3,8), (4,1), (5,5), (6,6), (7,3), (8,4), (9,7))
8
2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Thu, 21 Feb 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/spark/2019/02/21/spark-LSH.html</link>
        <guid isPermaLink="true">http://localhost:4000/spark/2019/02/21/spark-LSH.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>Spark Matrix</title>
        <description>&lt;h3 id=&quot;spark-matrix-計算-cosine-similarity&quot;&gt;spark matrix 計算 cosine Similarity&lt;/h3&gt;

&lt;p&gt;假設有一群人的資料，有每個人的 label 跟分數．將每個人的 label 與分數轉成向量後計算彼此的 cosine Similarity，透過 cosine Similarity 來看這些人的相似程度如何．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val personDatas = Seq(
  (&quot;person1&quot;,&quot;1:0.5,2:0.3,3:0.4&quot;) ,
  (&quot;person2&quot;,&quot;2:0.7&quot;) ,
  (&quot;person3&quot;,&quot;1:0.9,3:0.1&quot;) ,
  (&quot;person4&quot;,&quot;1:0.3,2:0.6,3:0.8&quot;)
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;person1 的 label 分數轉成 [0.5,0.3,0.4] 代表一個維度為 3 的向量．&lt;br /&gt;
person2 的 label 分數轉成 [0.0,0.7,0.0]．&lt;br /&gt;
然後用 cosine Similarity 的公式來計算 [0.5,0.3,0.4] 與 [0.0,0.7,0.0] 的 cosine Similarity value．&lt;/p&gt;

&lt;p&gt;接著將上面的資料轉成 IndexedRowMatrix．透過 RDD 的 zipWithIndex，可以取得每個元素的 index 從 0 開始 :&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val comparePersons = spark.sparkContext.parallelize(personDatas).toDF(&quot;id&quot;,&quot;labels&quot;).cache()

val allPerson = comparePersons.rdd.zipWithIndex.map {
  case (row , index) =&amp;gt; {
    val id = row.getAs[String](&quot;id&quot;)
    val labels = row.getAs[String](&quot;labels&quot;).split(&quot;,&quot;)
    val lindexs = labels.map(lstr =&amp;gt; (lstr.split(&quot;:&quot;)(0).toInt - 1))
    val lvalues = labels.map(lstr =&amp;gt; lstr.split(&quot;:&quot;)(1).toDouble)
    val labelVector =  org.apache.spark.mllib.linalg.Vectors.sparse(4, lindexs, lvalues)
    (id , new IndexedRow(index , labelVector) )
  }
}.cache()

val indexRowMatrix = new IndexedRowMatrix(allPerson.map(_._2))

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;將 IndexedRowMatrix 轉成 CoordinateMatrix 後轉置(transpose)，然後再轉成 IndexedRowMatrix，
利用 IndexedRowMatrix 的 columnSimilarities 來幫忙算出每個向量之間的相似度 (cosine similarity)．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val newMatrix = indexRowMatrix.toCoordinateMatrix.transpose.toIndexedRowMatrix()
val newCosValues = newMatrix.columnSimilarities()
newMatrix.rows.foreach(println(_))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;印出的結果會是&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;MatrixEntry(0,1,0.42426406871192845)
MatrixEntry(0,2,0.7652514332541697)
MatrixEntry(0,3,0.8804710999221752)
MatrixEntry(1,3,0.5746957711326908)
MatrixEntry(2,3,0.37020976437050546)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;columnsimilarities-使用說明&quot;&gt;columnSimilarities 使用說明&lt;/h3&gt;
&lt;p&gt;1.原來的矩陣&lt;br /&gt;
[0.5 , 0.3 , 0.4 , 0]&lt;br /&gt;
[0 , 0.7 , 0 , 0]&lt;br /&gt;
[0.9 , 0 , 0.1 , 0]&lt;br /&gt;
如果沒轉置使用 columnSimilarities 的話，結果會是&lt;br /&gt;
[0.5 , 0 , 0.9] 跟 [0.3 , 0.7 , 0] 的相似度 0.19130412280981776&lt;br /&gt;
[0.5 , 0 , 0.9] 跟 [0.4 , 0 , 0.1] 的相似度 0.6831571287757409&lt;br /&gt;
[0.5 , 0 , 0.9] 跟 [0 , 0 , 0] 的相似度 NaN (無法計算不顯示)&lt;br /&gt;
[0.3 , 0.7 , 0] 跟 [0.4 , 0 , 0.1] 的相似度 0.3821578531790892&lt;br /&gt;
[0.3 , 0.7 , 0] 跟 [0 , 0 , 0] 的相似度 NaN (無法計算不顯示)&lt;br /&gt;
[0.4 , 0 , 0.1] 跟 [0 , 0 , 0] 的相似度 NaN (無法計算不顯示)
這樣並不是正確的結果，因為希望的是上面三個向量彼此的相似度．所以要將矩陣轉置．&lt;/p&gt;

&lt;p&gt;2.轉置後的矩陣&lt;br /&gt;
IndexedRow(0,[0.5,0.0,0.9])&lt;br /&gt;
IndexedRow(1,[0.3,0.7,0.0])&lt;br /&gt;
IndexedRow(2,[0.4,0.0,0.1])&lt;br /&gt;
IndexedRow(3,[0.0,0.0,0.0])&lt;br /&gt;
使用 columnSimilarities 的話，結果會是&lt;br /&gt;
[0.5 , 0.3 , 0.4 , 0] 跟 [0 , 0.7 , 0 , 0] 的相似度 0.42426406871192845&lt;br /&gt;
[0.5 , 0.3 , 0.4 , 0] 跟 [0.9 , 0 , 0.1 , 0] 的相似度 0.7652514332541697&lt;br /&gt;
[0 , 0.7 , 0 , 0] 跟  [0.9 , 0 , 0.1 , 0] 的相似度 0 (相似度 0 的話就不顯示)&lt;br /&gt;
可以用下列的 cosineSimilarityVerifyTest 來驗證相似度是否正確．計算兩個向量的 cosine Similarity，越大代表越像．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;test(&quot;cosineSimilarityVerifyTest&quot;) {
	//[0.5 , 0 , 0.9] 跟 [0 , 0 , 0] 的相似度 0
	val query = List[Double](0.5 , 0.3 , 0.4 , 0)
	val labels = List[Double](0 , 0.7 , 0 , 0)
	val cv1 = cosineSimilarity(query.toArray , labels.toArray)
	println(&quot;cv1 : &quot; + cv1) // cv1 : 0.42426406871192845
}

def cosineSimilarity(x: Array[Double], y: Array[Double]): Double = {
require(x.size == y.size)
genDot(x, y)/(magnitude(x) * magnitude(y))
}

def genDot(x: Array[Double], y: Array[Double]): Double = {
(for((a, b) &amp;lt;- x.zip(y)) yield a * b).sum
}

def magnitude(x: Array[Double]): Double = {
math.sqrt(x.map(i =&amp;gt; i*i).sum)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;其他參考作法&quot;&gt;其他參考作法&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val spark = SparkSession.builder()
  .master(&quot;local[*]&quot;)
  .appName(&quot;testtest&quot;)
  .getOrCreate()

import spark.implicits._

val testSeq = Seq(
  (&quot;1&quot;,&quot;1:0.5,2:0.3,3:0.4&quot;) ,
  (&quot;2&quot;,&quot;2:0.7&quot;) ,
  (&quot;3&quot;,&quot;1:0.9,3:0.1&quot;)
)
val rddEntrys = testSeq.map {
  case(i , labels) =&amp;gt; {
    val entrys = labels.split(&quot;,&quot;).map(l =&amp;gt; {
      val index = l.split(&quot;:&quot;)(0).toInt - 1
      val v = l.split(&quot;:&quot;)(1).toDouble
      new MatrixEntry(index , (i.toLong - 1) , v )
    })
    entrys
  }
}.flatten

val temp = spark.sparkContext.parallelize(rddEntrys)

val corMatrix = new CoordinateMatrix(temp)
corMatrix.entries.foreach(println(_))

val cv = corMatrix.toIndexedRowMatrix().columnSimilarities()
cv.entries.foreach(println(_))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;印出結果&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;MatrixEntry(0,1,0.42426406871192845)
MatrixEntry(0,2,0.7652514332541697)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val spark = SparkSession.builder()
  .master(&quot;local[*]&quot;)
  .appName(&quot;testtest&quot;)
  .getOrCreate()

import spark.implicits._

val testSeq = Seq(
  (&quot;1&quot;,&quot;1:0.5,2:0.3,3:0.4&quot;) ,
  (&quot;2&quot;,&quot;2:0.7&quot;) ,
  (&quot;3&quot;,&quot;1:0.9,3:0.1&quot;)
)
val rddEntrys = testSeq.map {
  case(i , labels) =&amp;gt; {
    val entrys = labels.split(&quot;,&quot;).map(l =&amp;gt; {
      val index = l.split(&quot;:&quot;)(0).toInt - 1
      val v = l.split(&quot;:&quot;)(1).toDouble
      (index , ((i.toLong - 1).toInt , v) )
    })
    entrys
  }
}.flatten

val indexedRows = spark.sparkContext.parallelize(rddEntrys).groupByKey.map {
  case(i, vectorEntries) =&amp;gt; {
    IndexedRow(i, Vectors.sparse(3, vectorEntries.toSeq))
  }
}
val numRows = indexedRows.count

val cv = new IndexedRowMatrix(indexedRows, numRows, 3).columnSimilarities()
cv.entries.foreach(println(_))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;印出結果&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;MatrixEntry(0,1,0.42426406871192845)
MatrixEntry(0,2,0.7652514332541697)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Wed, 20 Feb 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/spark/2019/02/20/spark-matrix.html</link>
        <guid isPermaLink="true">http://localhost:4000/spark/2019/02/20/spark-matrix.html</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>Scala</title>
        <description>&lt;h2 id=&quot;scala-tutorial-template&quot;&gt;scala tutorial template&lt;/h2&gt;

&lt;p&gt;scala template&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Feb 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/scala/2019/02/13/scala.html</link>
        <guid isPermaLink="true">http://localhost:4000/scala/2019/02/13/scala.html</guid>
        
        
        <category>scala</category>
        
      </item>
    
      <item>
        <title>kafka install</title>
        <description>&lt;p&gt;Kafka Install&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Feb 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/kafka/2019/02/13/kafka-install.html</link>
        <guid isPermaLink="true">http://localhost:4000/kafka/2019/02/13/kafka-install.html</guid>
        
        
        <category>kafka</category>
        
      </item>
    
      <item>
        <title>hello jekyll</title>
        <description>&lt;h4 id=&quot;目錄架構&quot;&gt;目錄架構&lt;/h4&gt;

&lt;p&gt;jekyll 的目錄結構如下，文章主要放在 _post 目錄裡，圖片、js、css 之類的放在 static 目錄裡．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/img/jekyll_1.jpg&quot; alt=&quot;jekyll_1.jpg&quot; height=&quot;400px&quot; width=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;寫文章時要先定義好標頭，layout 會對應到 _layouts 目錄，categories 用來對文章進行分類．&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
layout: post
title:  &quot;hello jekyll&quot;
date:   2015-02-10 15:14:54
categories: jekyll
comments: true
---

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;再使用 &lt;a href=&quot;https://pages.github.com/&quot;&gt;git page&lt;/a&gt;，把 project commit 到 repositories，畫面結果如下．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/img/jekyll_2.jpg&quot; alt=&quot;jekyll_2.jpg&quot; height=&quot;400px&quot; width=&quot;600px&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 10 Feb 2015 23:14:54 +0800</pubDate>
        <link>http://localhost:4000/jekyll/2015/02/10/welcome-to-jekyll.html</link>
        <guid isPermaLink="true">http://localhost:4000/jekyll/2015/02/10/welcome-to-jekyll.html</guid>
        
        
        <category>jekyll</category>
        
      </item>
    
  </channel>
</rss>
