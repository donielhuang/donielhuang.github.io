<!DOCTYPE html>
<html>

  <head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Zeppelin Helloword</title>
	<meta name="description" content="zeppelin">
	
	<link rel="canonical" href="http://localhost:4000/spark/2019/03/07/zeppelin-helloword.html">
	<link rel="alternate" type="application/rss+xml" title="Daniel's Blog" href="http://localhost:4000/feed.xml" />
	
	<!-- <link rel="stylesheet" href="/css/main.css"> -->
    
    <link rel="stylesheet" type="text/css" href="/static/css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="/static/css/index.css">
	<script type="text/javascript" src="/static/js/jquery-1.11.1.min.js"></script>
	<script type="text/javascript" src="/static/js/bootstrap.min.js"></script>
    <link rel="stylesheet" type="text/css" href="/static/css/monokai_sublime.min.css">
	<script type="text/javascript" src="/static/js/highlight.min.js"></script>

    <!--
    <link rel="stylesheet" type="text/css" href="http://apps.bdimg.com/libs/bootstrap/3.3.0/css/bootstrap.min.css">
	<script type="text/javascript" src="http://apps.bdimg.com/libs/jquery/2.1.1/jquery.min.js"></script>
	<script type="text/javascript" src="http://apps.bdimg.com/libs/bootstrap/3.3.0/js/bootstrap.min.js"></script>
    <link rel="stylesheet" type="text/css" href="http://apps.bdimg.com/libs/highlight.js/8.4/styles/monokai_sublime.min.css">
	<script type="text/javascript" src="http://apps.bdimg.com/libs/highlight.js/8.4/highlight.min.js"></script>
    -->
    
	<script type="text/javascript" src="/static/js/index.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
</head>

 <!--  <body data-spy="scroll" data-target="#myAffix"> -->
  <body>

    <header>

<!-- navbar -->
  <nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Daniel's Blog</a>
      <p class="navbar-text"></p>
    </div>
    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav navbar-right">

        
          <li>
        
        <a href="/">Home</a></li>

        
          
            
              <li>
            
            <a href="/projects/"><span class="glyphicon "></span> Projects</a></li>
          
        
          
            
              <li>
            
            <a href="/about/"><span class="glyphicon "></span> About</a></li>
          
        
          
        
          
        
      </ul>
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

</header>

    <div id="main" class="container main">
      <div class="row">
  <div id="myArticle" class="col-sm-9">
    <div class="post-area post">
      <header>
        <h1>Zeppelin Helloword</h1>
        <p>Mar 7, 2019</p>
      </header>
      <hr>
      <article>
        <h3 id="zeppelin">zeppelin</h3>

<p>使用 zeppelin 透過 spark 對 cassandra 做查詢．</p>

<p>先用 docker 下載後，再開 http://localhost:8080</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker pull apache/zeppelin:0.8.1
docker run -p 8080:8080 --rm apache/zeppelin:0.8.1
</code></pre></div></div>

<p>建立 notebook</p>

<p><img src="/static/img/zeppelin/zeppelin_1.jpg" alt="zeppelin_1.jpg" height="80%" width="80%" /></p>

<p>原本程式的寫法 :</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val spark: SparkSession = SparkSession.builder().appName("KmeansLookLikePerson")
      .config("spark.sql.warehouse.dir", "/tmp")
      .config("spark.cassandra.connection.host", EnrichConfig.CASSANDRA_IPS)
      .config("spark.cassandra.auth.username", EnrichConfig.CASSANDRA_USER)
      .config("spark.cassandra.auth.password", EnrichConfig.CASSANDRA_PSWD)
      .getOrCreate()

val allPersons = spark.sparkContext.cassandraTable[(String,Seq[Double])]("miks2","testpersonlabelvector")
  .select("id","labelvector").keyBy[Tuple1[String]]("id")

</code></pre></div></div>

<p>寫到 notebook 裡面，可參考 <a href="https://zeppelin.apache.org/docs/0.7.2/interpreter/spark.html#sparkcontext-sqlcontext-sparksession-zeppelincontext">spark Interpreter for zeppelin</a>，
所以 zeppelin 預設就會給一個 sc 就是 sparkContext．</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import com.datastax.spark.connector._

val allPersons = sc.cassandraTable[(String,Seq[Double])]("miks2","testpersonlabelvector")
      .select("id","labelvector").keyBy[Tuple1[String]]("id")

allPersons.take(5).foreach(p =&gt; println(p._1._1))

</code></pre></div></div>

<p>那原本要設定 cassandra 的 host、username、password 要帶給 sparkContext，就要設定在 Interpreter 的 spark 設定裡．</p>

<p><img src="/static/img/zeppelin/zeppelin_2.jpg" alt="zeppelin_2.jpg" height="80%" width="80%" /></p>

<p>把參數設定在這裡 :</p>

<p><img src="/static/img/zeppelin/zeppelin_3.jpg" alt="zeppelin_3.jpg" height="80%" width="80%" /></p>

<p>那由於有使用到 spark cassandra 所以要 import library</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import com.datastax.spark.connector._
</code></pre></div></div>

<p>設定在 dependencies，加上需要的 library．</p>

<p><img src="/static/img/zeppelin/zeppelin_4.jpg" alt="zeppelin_4.jpg" height="80%" width="80%" /></p>

<p>zeppelin 會去 maven 或指定的 repositories 下載 library．</p>

<p><img src="/static/img/zeppelin/zeppelin_5.jpg" alt="zeppelin_5.jpg" height="80%" width="80%" /></p>

<p>結著只要按 run 就可以了．</p>

<p><img src="/static/img/zeppelin/zeppelin_6.jpg" alt="zeppelin_6.jpg" height="80%" width="80%" /></p>

<p>如果需要有一些視覺化的圖表，則需要先把 Dataframe 轉成 spark sql 的 table．</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>looklikePersons.registerTempTable("looklikePersons")
allPersons.toDF().registerTempTable("allPersons")
seedPersons.toDF().registerTempTable("seedPersons")
</code></pre></div></div>

<p>程式碼如下 :</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import com.datastax.spark.connector._
import org.apache.spark.ml.clustering.KMeans
import org.apache.spark.ml.linalg.Vectors

val kcluster = 2
val outputPersons = 10

val testpersonSeed = "hash:9ef58b79-8aab-4e5b-bcf5-b8974991e599,hash:cd5d9c99-393b-4814-8f62-9aebcc31bd21,hash:a0c95fa1-fc44-44ff-b4ae-c407cd53f292,hash:a2ada011-53a1-4d27-9885-dbfd2b0cb969,hash:ce1283e2-a72a-4377-86f5-c03774f00641,hash:4241c150-cde7-4cb7-9306-10946cdbb639,hash:be9fcd09-a1f5-467a-9065-82a89c86af83,hash:48f3d940-f0bd-4300-9f8a-33c29c56ace6,hash:35dc9c3d-f04d-4b27-a5ce-8180ea9705ec,hash:57258600-55c5-4d94-8d15-8f754d8d14bc"

val allPersons = sc.cassandraTable[(String,Seq[Double])]("miks2","testpersonlabelvector")
      .select("id","labelvector").keyBy[Tuple1[String]]("id")

val limitPersons = spark.sparkContext.parallelize(allPersons.take(1000))

val seedPersons = sc.cassandraTable[(String,Seq[Double])]("miks2","testpersonlabelvector")
  .select("id","labelvector").where("id in ? " , testpersonSeed.split(",").toSeq).keyBy[Tuple1[String]]("id")

val tranDataSet = seedPersons.filter(_._2._2.size == 254).map(r =&gt; ( r._2._1 ,Vectors.dense(r._2._2.toArray))).toDF("id", "features")

val kmeans = new KMeans().setK(kcluster).setSeed(kcluster)
val model = kmeans.fit(tranDataSet)
val seedKmodel = sc.parallelize(model.clusterCenters)
val resultIdList = seedKmodel.cartesian(limitPersons)
  .map(ds =&gt; (ds._2._1._1 , cosineSimilarity(ds._1.toArray , ds._2._2._2)))
  .sortBy(_._2 , false).take(outputPersons)

val looklikePersons = sc.parallelize(resultIdList).toDF()
looklikePersons.registerTempTable("looklikePersons")
allPersons.toDF().registerTempTable("allPersons")
seedPersons.toDF().registerTempTable("seedPersons")

def cosineSimilarity(x: Seq[Double], y: Seq[Double]): Double = {
  val v = genDot(x, y)/(magnitude(x) * magnitude(y))
  if(v.isNaN) {
    0
  } else {
    v
  }
}

def genDot(x: Seq[Double], y: Seq[Double]): Double = {
  (for((a, b) &lt;- x.zip(y)) yield a * b).sum
}

def magnitude(x: Seq[Double]): Double = {
  math.sqrt(x.map(i =&gt; i*i).sum)
}
</code></pre></div></div>

<p>接著只要下 sql 按 run 就可以看到圖表</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>%sql
select * from looklikePersons 
</code></pre></div></div>

<p><img src="/static/img/zeppelin/zeppelin_7.jpg" alt="zeppelin_7.jpg" height="80%" width="80%" /></p>

<p>使用 spark 查詢 cassandra 然後建立一個 table 建立一個 spark sql 的 table，讓 user 用 sql 下查詢並看結果．<br />
先查詢要的 table 及欄位 :</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import com.datastax.spark.connector._

val allPersons = sc.cassandraTable("miks1","twmperson").select("id","labels").map(row =&gt; (row.get[String]("id") , row.get[String]("labels"))).toDF("id","labels")

allPersons.registerTempTable("allPersons")

</code></pre></div></div>

<p>使用 sql 查詢想要的 labels 來查看有多少人有這些 labels</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>%sql
select id,labels from allPersons where labels like '%36%' or labels like '%92%'
</code></pre></div></div>

<p>結果表示有 labels 36 跟 92 的人共有 241 個。</p>

<p><img src="/static/img/zeppelin/zeppelin_8.jpg" alt="zeppelin_8.jpg" height="80%" width="80%" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./bin/install-interpreter.sh --name "interpreter-name" --artifact org.apache.zeppelin:spark2-shims:0.8.0 
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/zeppelin-0.8.1-bin-all/bin&gt;./install-interpreter.sh --name "interpreter-name" --artifact org.apache.zeppelin:spark2-shims:0.8.0
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/miuser/enrich/zeppelin/zeppelin-0.8.1-bin-all/lib/interpreter/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/miuser/enrich/zeppelin/zeppelin-0.8.1-bin-all/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Install interpreter-name(org.apache.zeppelin:spark2-shims:0.8.0) to /home/miuser/enrich/zeppelin/zeppelin-0.8.1-bin-all/interpreter/interpreter-name ...
Interpreter interpreter-name installed under /home/miuser/enrich/zeppelin/zeppelin-0.8.1-bin-all/interpreter/interpreter-name.

1. Restart Zeppelin
2. Create interpreter setting in 'Interpreter' menu on Zeppelin GUI
3. Then you can bind the interpreter on your note
miuser@dmpn1:~/enrich/zeppelin/zeppelin-0.8.1-bin-all/bin&gt;ll
total 33

</code></pre></div></div>

<p>如果環境是 spark 1 是使用 sc :</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val allPersons = sc.cassandraTable[(String,Map[String,Double])]("miks1","twmperson").select("id","labels").filter(l =&gt; l._2.contains("92")).toDF("id","labels")
</code></pre></div></div>
<p>如果環境是 spark 2 記得改成使用 spark(sparksession)，否則會有 dependency 的問題 :</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import com.datastax.spark.connector._

val allPersons = spark.sparkContext.cassandraTable[(String,Map[String,Double])]("miks1","twmperson").select("id","labels").filter(l =&gt; l._2.contains("92")).toDF("id","labels")

allPersons.registerTempTable("allPersons")

</code></pre></div></div>
<p>這段很重要 :</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SparkContext, SQLContext and ZeppelinContext are automatically created and exposed as variable names sc, sqlContext and z, respectively, in Scala, Python and R environments. Staring from 0.6.1 SparkSession is available as variable spark when you are using Spark 2.x.
</code></pre></div></div>

<p>遇到下列的錯誤</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>java.lang.NoSuchMethodError: io.netty.buffer.PooledByteBufAllocator.metric()Lio/netty/buffer/PooledByteBufAllocatorMetric;
  at org.apache.spark.network.util.NettyMemoryMetrics.registerMetrics(NettyMemoryMetrics.java:80)
  at org.apache.spark.network.util.NettyMemoryMetrics.&lt;init&gt;(NettyMemoryMetrics.java:76)
  at org.apache.spark.network.client.TransportClientFactory.&lt;init&gt;(TransportClientFactory.java:109)
  at org.apache.spark.network.TransportContext.createClientFactory(TransportContext.java:99)
  at org.apache.spark.rpc.netty.NettyRpcEnv.&lt;init&gt;(NettyRpcEnv.scala:71)
  at org.apache.spark.rpc.netty.NettyRpcEnvFactory.create(NettyRpcEnv.scala:461)
  at org.apache.spark.rpc.RpcEnv$.create(RpcEnv.scala:57)
  at org.apache.spark.SparkEnv$.create(SparkEnv.scala:249)
  at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:175)
  at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:256)
  at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:423)
  at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2493)
  at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:933)
  at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:924)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:924)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at org.apache.zeppelin.spark.BaseSparkScalaInterpreter.spark2CreateContext(BaseSparkScalaInterpreter.scala:259)
  at org.apache.zeppelin.spark.BaseSparkScalaInterpreter.createSparkContext(BaseSparkScalaInterpreter.scala:178)
  at org.apache.zeppelin.spark.SparkScala211Interpreter.open(SparkScala211Interpreter.scala:89)
  at org.apache.zeppelin.spark.NewSparkInterpreter.open(NewSparkInterpreter.java:102)
  at org.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:62)
  at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)
  at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:616)
  at org.apache.zeppelin.scheduler.Job.run(Job.java:188)
  at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:140)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
</code></pre></div></div>

<p>zeppelin netty jar 的版本 :</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/zeppelin/zeppelin-0.8.1-bin-all/lib&gt;ll | grep netty-all
-rw-r--r--  1 miuser miuser  1779991 Oct 16 11:05 netty-all-4.0.23.Final.jar
</code></pre></div></div>

<p>spark-2.3.1 netty jar 的版本 :</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/opt/spark-2.3.1-bin-hadoop2.7/jars&gt;ll | grep netty-all
-rw-rw-r--  1 miuser miuser  3780056 Jun  2  2018 netty-all-4.1.17.Final.jar
</code></pre></div></div>

<p>所以把 spark 的 netty jar copy 到 zeppelin</p>

<p>接著遇到 Jackson 版本問題，一樣把 spark 的 jackson jar 覆蓋到 zeppelin．</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>com.fasterxml.jackson.databind.JsonMappingException: Incompatible Jackson version: 2.8.11-1
  at com.fasterxml.jackson.module.scala.JacksonModule$class.setupModule(JacksonModule.scala:64)
  at com.fasterxml.jackson.module.scala.DefaultScalaModule.setupModule(DefaultScalaModule.scala:19)
  at com.fasterxml.jackson.databind.ObjectMapper.registerModule(ObjectMapper.java:747)
  at org.apache.spark.util.JsonProtocol$.&lt;init&gt;(JsonProtocol.scala:59)
  at org.apache.spark.util.JsonProtocol$.&lt;clinit&gt;(JsonProtocol.scala)
  at org.apache.spark.scheduler.EventLoggingListener$.initEventLog(EventLoggingListener.scala:303)
  at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:128)
  at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:522)
  at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2493)
  at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:933)
  at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:924)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:924)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at org.apache.zeppelin.spark.BaseSparkScalaInterpreter.spark2CreateContext(BaseSparkScalaInterpreter.scala:259)
  at org.apache.zeppelin.spark.BaseSparkScalaInterpreter.createSparkContext(BaseSparkScalaInterpreter.scala:178)
  at org.apache.zeppelin.spark.SparkScala211Interpreter.open(SparkScala211Interpreter.scala:89)
  at org.apache.zeppelin.spark.NewSparkInterpreter.open(NewSparkInterpreter.java:102)
  at org.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:62)
  at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)
  at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:616)
  at org.apache.zeppelin.scheduler.Job.run(Job.java:188)
  at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:140)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/zeppelin/zeppelin-0.8.1-bin-all/lib&gt;ll | grep jackson
-rw-r--r--  1 miuser miuser     5990 Oct 16 14:40 google-http-client-jackson-1.23.0.jar
-rw-r--r--  1 miuser miuser     6675 Oct 16 14:40 google-http-client-jackson2-1.23.0.jar
-rw-r--r--  1 miuser miuser    55784 Oct 16 11:58 jackson-annotations-2.8.0.jar
-rw-r--r--  1 miuser miuser   282634 Oct 16 14:38 jackson-core-2.8.10.jar
-rw-r--r--  1 miuser miuser   232248 Oct 16 10:56 jackson-core-asl-1.9.13.jar
-rw-r--r--  1 miuser miuser  1247444 Oct 16 14:38 jackson-databind-2.8.11.1.jar
-rw-r--r--  1 miuser miuser    18336 Oct 16 10:57 jackson-jaxrs-1.9.13.jar
-rw-r--r--  1 miuser miuser   780664 Oct 16 10:56 jackson-mapper-asl-1.9.13.jar
-rw-r--r--  1 miuser miuser    34610 Oct 16 14:38 jackson-module-jaxb-annotations-2.8.10.jar
-rw-r--r--  1 miuser miuser    27084 Oct 16 10:57 jackson-xc-1.9.13.jar
-rw-r--r--  1 miuser miuser    73055 Oct 16 14:38 jersey-media-json-jackson-2.27.jar
/zeppelin/zeppelin-0.8.1-bin-all/lib&gt;rm -f jackson-*
/zeppelin/zeppelin-0.8.1-bin-all/lib&gt;cp /opt/spark-2.3.1-bin-hadoop2.7/jars/jackson-* .
/zeppelin/zeppelin-0.8.1-bin-all/lib&gt;ll | grep jackson
-rw-r--r--  1 miuser miuser     5990 Oct 16 14:40 google-http-client-jackson-1.23.0.jar
-rw-r--r--  1 miuser miuser     6675 Oct 16 14:40 google-http-client-jackson2-1.23.0.jar
-rw-rw-r--  1 miuser miuser    46986 Mar 12 11:35 jackson-annotations-2.6.7.jar
-rw-rw-r--  1 miuser miuser   258919 Mar 12 11:35 jackson-core-2.6.7.jar
-rw-rw-r--  1 miuser miuser   232248 Mar 12 11:35 jackson-core-asl-1.9.13.jar
-rw-rw-r--  1 miuser miuser  1165323 Mar 12 11:35 jackson-databind-2.6.7.1.jar
-rw-rw-r--  1 miuser miuser   320444 Mar 12 11:35 jackson-dataformat-yaml-2.6.7.jar
-rw-rw-r--  1 miuser miuser    18336 Mar 12 11:35 jackson-jaxrs-1.9.13.jar
-rw-rw-r--  1 miuser miuser   780664 Mar 12 11:35 jackson-mapper-asl-1.9.13.jar
-rw-rw-r--  1 miuser miuser    32612 Mar 12 11:35 jackson-module-jaxb-annotations-2.6.7.jar
-rw-rw-r--  1 miuser miuser    42858 Mar 12 11:35 jackson-module-paranamer-2.7.9.jar
-rw-rw-r--  1 miuser miuser   515645 Mar 12 11:35 jackson-module-scala_2.11-2.6.7.1.jar
-rw-rw-r--  1 miuser miuser    27084 Mar 12 11:35 jackson-xc-1.9.13.jar
-rw-r--r--  1 miuser miuser    73055 Oct 16 14:38 jersey-media-json-jackson-2.27.jar
</code></pre></div></div>

<p><a href="https://blog.csdn.net/a376554764/article/details/84672444">zeppelin 整 spark 問題</a></p>

<p>跑 yarn-cluster mode :</p>

<p><img src="/static/img/zeppelin/zeppelin_9.jpg" alt="zeppelin_9.jpg" height="80%" width="80%" /></p>

<p>把 spark-cassandra-connector_2.11-2.3.2.jar 丟到各台的 spark lib 底下 :</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/opt/spark-2.3.1-bin-hadoop2.7/jars&gt;ll | grep cassandra
-rw-r--r--  1 miuser miuser  8539058 Mar 12 15:28 spark-cassandra-connector_2.11-2.3.2.jar
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import com.datastax.spark.connector._

val findLabelIndex = "3"
val cassandraKeyspace = "test1"
val cassandraTable = "test"
val allPersons = spark.sparkContext.cassandraTable[(String,Map[String,Double])](cassandraKeyspace,cassandraTable).select("id","labels").filter(l =&gt; l._2.contains(findLabelIndex)).toDF("id","labels")

allPersons.registerTempTable("allPersons")

</code></pre></div></div>

<p>將 zeppelin 的 spark notebook 跑在 yarn 上面 :</p>

<p><img src="/static/img/zeppelin/zeppelin_10.jpg" alt="zeppelin_10.jpg" height="80%" width="80%" /></p>


      </article>
      <hr>
    </div>
  </div>
</div>
    </div>

    
    <div id="top" data-toggle="tooltip" data-placement="left" title="back to top">
      <a href="javascript:;">
        <div class="arrow"></div>
        <div class="stick"></div>
      </a>
    </div>

    <footer class="">
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <a href="mailto:"><span class="glyphicon glyphicon-envelope"></span> </a>
          <span class="point"> · </span>
          <span class="point"> · </span>
          <span class="point"> · </span>
          <span><a href="_posts/zeppelin/2019-03-07-zeppelin-helloword.md">View source</a></span>
          <span class="point"> · </span>
		      <span class="point"> · </span>
          <span class="point"> · </span>
          <span>&copy; 2019 Daniel</span>
      </div>
    </div>
  </div>
</footer>
  
  </body>
</html>
