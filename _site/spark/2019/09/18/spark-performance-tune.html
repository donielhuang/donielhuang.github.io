<!DOCTYPE html>
<html>

  <head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Spark Performance Tune</title>
	<meta name="description" content="TransformationsTransformations 的操作有分為 Narrow 和 Wide．">
	
	<link rel="canonical" href="http://localhost:4000/spark/2019/09/18/spark-performance-tune.html">
	<link rel="alternate" type="application/rss+xml" title="Daniel's Blog" href="http://localhost:4000/feed.xml" />
	
	<!-- <link rel="stylesheet" href="/css/main.css"> -->
    
    <link rel="stylesheet" type="text/css" href="/static/css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="/static/css/index.css">
	<script type="text/javascript" src="/static/js/jquery-1.11.1.min.js"></script>
	<script type="text/javascript" src="/static/js/bootstrap.min.js"></script>
    <link rel="stylesheet" type="text/css" href="/static/css/monokai_sublime.min.css">
	<script type="text/javascript" src="/static/js/highlight.min.js"></script>

    <!--
    <link rel="stylesheet" type="text/css" href="http://apps.bdimg.com/libs/bootstrap/3.3.0/css/bootstrap.min.css">
	<script type="text/javascript" src="http://apps.bdimg.com/libs/jquery/2.1.1/jquery.min.js"></script>
	<script type="text/javascript" src="http://apps.bdimg.com/libs/bootstrap/3.3.0/js/bootstrap.min.js"></script>
    <link rel="stylesheet" type="text/css" href="http://apps.bdimg.com/libs/highlight.js/8.4/styles/monokai_sublime.min.css">
	<script type="text/javascript" src="http://apps.bdimg.com/libs/highlight.js/8.4/highlight.min.js"></script>
    -->
    
	<script type="text/javascript" src="/static/js/index.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
</head>

 <!--  <body data-spy="scroll" data-target="#myAffix"> -->
  <body>

    <header>

<!-- navbar -->
  <nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Daniel's Blog</a>
      <p class="navbar-text"></p>
    </div>
    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav navbar-right">

        
          <li>
        
        <a href="/">Home</a></li>

        
          
            
              <li>
            
            <a href="/projects/"><span class="glyphicon "></span> Projects</a></li>
          
        
          
            
              <li>
            
            <a href="/about/"><span class="glyphicon "></span> About</a></li>
          
        
          
        
          
        
      </ul>
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

</header>

    <div id="main" class="container main">
      <div class="row">
  <div id="myArticle" class="col-sm-9">
    <div class="post-area post">
      <header>
        <h1>Spark Performance Tune</h1>
        <p>Sep 18, 2019</p>
      </header>
      <hr>
      <article>
        <h2 id="transformations">Transformations</h2>
<p>Transformations 的操作有分為 Narrow 和 Wide．</p>

<h4 id="narrow-transformations">Narrow Transformations</h4>
<p>表示從 parent RDD 經過 Narrow Transformations 的轉換後，還是只會成為單一的 partition．屬於一對一的關係，所以 partition 的資料不會跨 partition 造成 shuffle．
相關的操作有 map , filter , union．</p>

<h4 id="wide-transformations">Wide Transformations</h4>
<p>表示從 parent RDD 經過 Wide Transformations 的轉換後，原來 partition 的資料會經過 shuffle 分散到不同的 partition 去，屬於一對多的關係．相關操作有 groupByKey、reduceByKey、distinct、join．</p>

<h2 id="resource-allocation">Resource Allocation</h2>

<p>影響到 spark application 的效能需要考慮到 CPU、memory、diosk IO、network IO 當然還有程式的寫法盡量避免太多 shuffle．</p>

<p>當要執行一個 spark application 時，這邊使用的是跑在 hadoop cluster 的 yarn 上面．需要根據 cluster 的資源來思考要怎麼分配給 spark executor 使用作運算，指令如下</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-submit --class com.mitwm.poc.CustomOperate --master yarn --deploy-mode cluster --driver-memory 4g --executor-memory 15g --executor-cores 5 --num-executors 12 enrich-5.0.3.jar 
</code></pre></div></div>

<h4 id="core">core</h4>

<p>core 數影響的是 executor task 的並行執行的數量，所以 ` –executor-cores 5 ` 表示一個 executor 最多能有 5 個 task 同時在執行．另外 HDFS client 最好的 throughput 是一個 executor 最多 5 個 task．</p>

<h4 id="memory">memory</h4>

<p>memory 影響的是能有多少量的資料可以存放在 executor 裡，存放的類型有 cache data、shuffle data．但要注意設太大時 GC 的時間可能也會增加太多．一個 executor 建議的最大值不要超過 64 G．</p>

<h4 id="evaluate-executor-resource">evaluate executor resource</h4>

<p>假設有 6 台機器，每台機器有 16 cores 和 64 G 的 memory．可以用 15 個 cores 和 63 G 的資源來做 hadoop cluster．<br />
所以 hadoop cluster 的資源總共會有 15 * 6 = 90 cores 和 63 G * 6 = 378 G 的 memory．<br />
這時要設定 yarn 的兩個參數  yarn.nodemanager.resource.memory-mb 和  yarn.nodemanager.resource.cpu-vcores．</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>yarn.nodemanager.resource.memory-mb 設定 63 G * 1024 = 64512 M．
yarn.nodemanager.resource.cpu-vcores 設定 15．
</code></pre></div></div>

<p>根據上面的資源情況，spark executor 的資源這樣分配，每台跑一個 executor 每個 executor 15 cores 和 63G 的 memory．</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--num-executors 6 --executor-cores 15 --executor-memory 63G
</code></pre></div></div>

<p>看似合理但其實會有遇到一些問題，</p>
<ul>
  <li>因為 executor memory 還會需要一些 overhead，所以當 63G 的 memory 都給 executor 使用就沒有多的 memory 給 overhead．</li>
  <li>spark driver 會被丟到其中一台機器執行，但也沒有額外多的 core 和 memory 給 driver 使用了．</li>
  <li>HDFS IO throughput 最好的是一個 executor 給 5 個 core．</li>
</ul>

<h4 id="executor-cores">executor-cores</h4>
<p>要改善上面的值 core 數就給 5 因為 HDFS IO throughput 最好．</p>

<h4 id="num-executors">num-executors</h4>
<p>總共有 90 個 cores / 5 (一個 executor) = 18 (總共最多可以有 18 個 executor) - 1 (for driver) = 17 executor (17 個 executor 平行度最佳)</p>

<h4 id="executor-memory">executor-memory</h4>
<p>15(一台機器 15 core) / 5(一個 executor) = 3 (一台機器最多能有 3 個 executor)</p>

<p>63 G / 3 = 21 G．
計算 overhead memory : 21 G * 0.1 = 2.1 G，大於 384 M．所以 overhead memory 是 2.1 G．
最後將 21 G - 2.1 G = 18.9 ~ 18 G．<br />
記得 driver memory 也會有 overhead memory 也是用這樣的方式算．假設 driver memroy 給 5 G，overhead 就是 5 * 0.1 = 0.5 G，實際用到的 memory 會是 5.5 G．<br />
0.1 這個值是參考 <a href="https://spark.apache.org/docs/2.3.1/configuration.html#viewing-spark-properties">spark 2.3.1 configuration</a> 來的，建議不同版本的話要確認一下．</p>

<p>所以最後 spark executor 資源分配改成</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--num-executors 17 --executor-cores 5 --executor-memory 18G
</code></pre></div></div>

<p>但實際上測試時如果 executor-memory 造上面的公式計算，job 還是有可能會 failed，感覺還是要多留一些 buffer ．所以如果出現 failed 18 G 在降個 1 ~ 2 G 試試看．</p>

<h4 id="partitions">partitions</h4>
<p>spark 會把要處理的資料分散在不同的 partition 裡，一個 task 可以處理一個 partition．partition 的切法可以透過 <code class="highlighter-rouge">spark.default.parallelism</code> 值設定．<br />
如果是讀取 HDFS 的檔案的話則會根據 HDFS 的 block size 來當作 partitions，所以 HDFS 的 block size 如果是 128 M，則 spark 讀取後每個 partition 也就是 128 M．(128<code class="highlighter-rouge">*</code>1024<code class="highlighter-rouge">*</code>1024 , such as 134217728 for 128 MB).</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;property&gt;
      &lt;name&gt;dfs.blocksize&lt;/name&gt;
      &lt;value&gt;134217728&lt;/value&gt;
&lt;/property&gt;
</code></pre></div></div>
<p>透過下圖可以看到讀取 HDFS，Input size 的話幾乎都是 128 M，但如果 HDFS 的檔案小於 128 M，例如 279K 則這 279 K 就會是個自一個 partition．</p>

<p><img src="/static/img/spark/sparkPerformance/sparkPerformance_1.jpg" alt="sparkPerformance_1.jpg" height="400px" width="800px" /></p>

<p>如果要確定 rdd 的 partition 數量可以用下面方式．</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rdd.partitions().size()
</code></pre></div></div>

<p>如果 partition 數量太少，假設有 10 台機器，但 partition 卻只有 8 個的情況下，代表不會所有的機器都會用到，會造成 cpu 的浪費．而且表示這些 small task 如果使用到一些 aggregation 的操作，例如 join、reduceByKey 之類的，而且這些操作會造成 shuffle，所以在 shuffle 時 memory 會需要比較多的，如果沒分配好會容易會造成 job failed．<br />
所以如果想要增加 partition 數量時可以使用下列方式．</p>
<ul>
  <li>使用 repartition 重新分配 partition 的數量．</li>
  <li>將 HDFS 的 block size 設小一點．</li>
</ul>

<p>如果要 repartition，但需要知道要分成多少個 partition 比較適當．</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val rdd2 = rdd1.reduceByKey(_ + _, numPartitions = X)
</code></pre></div></div>

<p>比較好的 performance 是根據 parent RDD 的 partition 數乘上 1.5 倍．</p>

<p>所以如果讀取 HDFS 檔案切成 119 個 partition * 1.5 = 178.5，在做 reduceByKey 時 X 值可以設成 178 看看效能是否能增加．</p>

<p>在通常的情況下多一點 partition 會比太少的 partition 好．這個建議跟 MapReduce 剛好相反, 因為 spark 啟動 tasks 的 overhead 相對較少．</p>


      </article>
      <hr>
    </div>
  </div>
</div>
    </div>

    
    <div id="top" data-toggle="tooltip" data-placement="left" title="back to top">
      <a href="javascript:;">
        <div class="arrow"></div>
        <div class="stick"></div>
      </a>
    </div>

    <footer class="">
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <a href="mailto:"><span class="glyphicon glyphicon-envelope"></span> </a>
          <span class="point"> · </span>
          <span class="point"> · </span>
          <span class="point"> · </span>
          <span><a href="_posts/spark/2019-09-18-spark-performance-tune.md">View source</a></span>
          <span class="point"> · </span>
		      <span class="point"> · </span>
          <span class="point"> · </span>
          <span>&copy; 2019 Daniel</span>
      </div>
    </div>
  </div>
</footer>
  
  </body>
</html>
